{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset & Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qbE-4ZdtY7N",
    "outputId": "150e2d3c-afbb-46e9-a049-df0f1e14daeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OrgId</th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>AlertId</th>\n",
       "      <th>DetectorId</th>\n",
       "      <th>AlertTitle</th>\n",
       "      <th>IncidentGrade</th>\n",
       "      <th>EvidenceRole</th>\n",
       "      <th>DeviceId</th>\n",
       "      <th>Sha256</th>\n",
       "      <th>...</th>\n",
       "      <th>Category_Impact</th>\n",
       "      <th>Category_InitialAccess</th>\n",
       "      <th>Category_Other</th>\n",
       "      <th>Category_SuspiciousActivity</th>\n",
       "      <th>EntityType_CloudLogonRequest</th>\n",
       "      <th>EntityType_Ip</th>\n",
       "      <th>EntityType_MailMessage</th>\n",
       "      <th>EntityType_Mailbox</th>\n",
       "      <th>EntityType_Other</th>\n",
       "      <th>EntityType_User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>455266534868</td>\n",
       "      <td>88</td>\n",
       "      <td>326</td>\n",
       "      <td>210035</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056561957389</td>\n",
       "      <td>809</td>\n",
       "      <td>58352</td>\n",
       "      <td>712507</td>\n",
       "      <td>423</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214748368522</td>\n",
       "      <td>148</td>\n",
       "      <td>4359</td>\n",
       "      <td>188041</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073741827836</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>831157</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>223338299440</td>\n",
       "      <td>6</td>\n",
       "      <td>2472</td>\n",
       "      <td>1148</td>\n",
       "      <td>17</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id  OrgId  IncidentId  AlertId  DetectorId  AlertTitle  \\\n",
       "0   455266534868     88         326   210035          58          43   \n",
       "1  1056561957389    809       58352   712507         423         298   \n",
       "2   214748368522    148        4359   188041           9          74   \n",
       "3  1073741827836     72          70   831157           4           3   \n",
       "4   223338299440      6        2472     1148          17         284   \n",
       "\n",
       "   IncidentGrade  EvidenceRole  DeviceId  Sha256  ...  Category_Impact  \\\n",
       "0              0             0     98799  138268  ...            False   \n",
       "1              0             1     98799  138268  ...            False   \n",
       "2              2             0     98799  138268  ...            False   \n",
       "3              2             0     98799  138268  ...            False   \n",
       "4              0             1     98799  138268  ...             True   \n",
       "\n",
       "   Category_InitialAccess  Category_Other  Category_SuspiciousActivity  \\\n",
       "0                   False           False                        False   \n",
       "1                    True           False                        False   \n",
       "2                   False            True                        False   \n",
       "3                    True           False                        False   \n",
       "4                   False           False                        False   \n",
       "\n",
       "   EntityType_CloudLogonRequest  EntityType_Ip  EntityType_MailMessage  \\\n",
       "0                         False          False                   False   \n",
       "1                         False          False                   False   \n",
       "2                         False          False                   False   \n",
       "3                         False          False                   False   \n",
       "4                         False           True                   False   \n",
       "\n",
       "   EntityType_Mailbox  EntityType_Other  EntityType_User  \n",
       "0               False             False             True  \n",
       "1               False              True            False  \n",
       "2               False             False             True  \n",
       "3               False             False             True  \n",
       "4               False             False            False  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../NoteBooks/cleaned_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3437166\n",
      "Validation set size: 1473072\n",
      "\n",
      "Class distribution in training set:\n",
      "IncidentGrade\n",
      "2    0.403098\n",
      "1    0.388515\n",
      "0    0.208387\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "IncidentGrade\n",
      "2    0.403098\n",
      "1    0.388515\n",
      "0    0.208387\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['IncidentGrade'])  # Features\n",
    "y = df['IncidentGrade']                 # Target variable   \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 171857\n",
      "Validation set size: 73654\n",
      "\n",
      "Class distribution in training set:\n",
      "IncidentGrade\n",
      "2    0.403097\n",
      "1    0.388515\n",
      "0    0.208388\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "IncidentGrade\n",
      "2    0.403101\n",
      "1    0.388519\n",
      "0    0.208380\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Reducing the dataset size\n",
    "sample_fraction = 0.05  # Adjust as needed (e.g., 10% of the original dataset)\n",
    "df_sampled, _ = train_test_split(\n",
    "    df, \n",
    "    stratify=df['IncidentGrade'], \n",
    "    test_size=1-sample_fraction, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['IncidentGrade'])\n",
    "y = df_sampled['IncidentGrade']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Function to apply balancing in chunks\n",
    "def balance_data(X, y, chunk_size=50000, random_state=42):\n",
    "    balanced_chunks = []  # To store balanced chunks\n",
    "    smote = SMOTE(random_state=random_state, n_jobs=-1)  # Initialize SMOTE\n",
    "    smote_enn = SMOTEENN(random_state=random_state, n_jobs=-1)  # Initialize SMOTE-ENN\n",
    "\n",
    "    for i in range(0, len(X), chunk_size):  # Process data in chunks\n",
    "        X_chunk, y_chunk = X[i:i + chunk_size], y[i:i + chunk_size]\n",
    "\n",
    "        # Step 1: Apply SMOTE to oversample minority classes\n",
    "        X_smote, y_smote = smote.fit_resample(X_chunk, y_chunk)\n",
    "\n",
    "        # Step 2: Apply SMOTEENN to clean noisy samples and further balance\n",
    "        X_res, y_res = smote_enn.fit_resample(X_smote, y_smote)\n",
    "\n",
    "        # Store balanced chunk\n",
    "        balanced_chunks.append((pd.DataFrame(X_res), pd.Series(y_res)))\n",
    "\n",
    "    # Combine all balanced chunks into a single dataset\n",
    "    X_resampled = pd.concat([chunk[0] for chunk in balanced_chunks], ignore_index=True)\n",
    "    y_resampled = pd.concat([chunk[1] for chunk in balanced_chunks], ignore_index=True)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Applying the function to the dataset\n",
    "X_resampled, y_resampled = balance_data(X_train, y_train, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 69275, 1: 66769, 0: 35813})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 41238, 0: 36672, 1: 20121})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_test, y_resampled_test = balance_data(X_test, y_test, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 29690, 1: 28616, 0: 15348})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 17403, 0: 15360, 1: 8315})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_test))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m numerical_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrgId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncidentId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlertId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetectorId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlertTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeviceId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSha256\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIpAddress\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUrl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccountSid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccountUpn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccountObjectId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccountName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeviceName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNetworkMessageId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApplicationId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApplicationName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFileName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFolderPath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResourceIdName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountryCode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fit the scaler on the training data and transform both train and test datasets\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m X_resampled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_resampled\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumerical_features\u001b[49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Compute and apply scaling on training data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m X_resampled_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_resampled_test[numerical_features])       \u001b[38;5;66;03m# Apply the same transformation on test data\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# import scaling 4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = ['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'AlertTitle', 'DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn', 'AccountObjectId', 'AccountName', 'DeviceName', 'NetworkMessageId', 'ApplicationId', 'ApplicationName', 'FileName', 'FolderPath', 'ResourceIdName', 'CountryCode', 'State', 'City']\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test datasets\n",
    "X_resampled = scaler.fit_transform(X_resampled[numerical_features])  # Compute and apply scaling on training data\n",
    "X_resampled_test = scaler.transform(X_resampled_test[numerical_features])       # Apply the same transformation on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Performance on Test Set:\n",
      "Macro-F1 Score: 0.6531\n",
      "Precision: 0.7168\n",
      "Recall: 0.6542\n",
      "Accuracy: 0.7312\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72     15360\n",
      "           1       0.67      0.27      0.38      8315\n",
      "           2       0.84      0.87      0.85     17403\n",
      "\n",
      "    accuracy                           0.73     41078\n",
      "   macro avg       0.72      0.65      0.65     41078\n",
      "weighted avg       0.73      0.73      0.71     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[30219  2510  3943]\n",
      " [11827  5280  3014]\n",
      " [ 5328   328 35582]]\n",
      "Class 0: TP=30219, FP=17155, FN=6453, TN=44204\n",
      "Class 1: TP=5280, FP=2838, FN=14841, TN=75072\n",
      "Class 2: TP=35582, FP=6957, FN=5656, TN=49836\n",
      "--------------------------------------------------\n",
      "Training Decision Tree...\n",
      "Decision Tree Performance on Test Set:\n",
      "Macro-F1 Score: 0.9404\n",
      "Precision: 0.9383\n",
      "Recall: 0.9430\n",
      "Accuracy: 0.9488\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     15360\n",
      "           1       0.88      0.92      0.90      8315\n",
      "           2       0.98      0.98      0.98     17403\n",
      "\n",
      "    accuracy                           0.95     41078\n",
      "   macro avg       0.94      0.94      0.94     41078\n",
      "weighted avg       0.95      0.95      0.95     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[34912  1377   383]\n",
      " [ 1256 18438   427]\n",
      " [  325   380 40533]]\n",
      "Class 0: TP=34912, FP=1581, FN=1760, TN=59778\n",
      "Class 1: TP=18438, FP=1757, FN=1683, TN=76153\n",
      "Class 2: TP=40533, FP=810, FN=705, TN=55983\n",
      "--------------------------------------------------\n",
      "Training Random Forest...\n",
      "Random Forest Performance on Test Set:\n",
      "Macro-F1 Score: 0.9572\n",
      "Precision: 0.9543\n",
      "Recall: 0.9603\n",
      "Accuracy: 0.9638\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     15360\n",
      "           1       0.91      0.95      0.93      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.96     41078\n",
      "   macro avg       0.95      0.96      0.96     41078\n",
      "weighted avg       0.96      0.96      0.96     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[35790   781   101]\n",
      " [  927 19065   129]\n",
      " [  294   426 40518]]\n",
      "Class 0: TP=35790, FP=1221, FN=882, TN=60138\n",
      "Class 1: TP=19065, FP=1207, FN=1056, TN=76703\n",
      "Class 2: TP=40518, FP=230, FN=720, TN=56563\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, f1_score, \n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    }\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Perform cross-validation predictions\n",
    "    y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)\n",
    "    \n",
    "    # Train on full training data\n",
    "    model.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Predictions on test data\n",
    "    y_val_pred = model.predict(X_resampled_test)\n",
    "    \n",
    "    # Evaluation metrics on test data\n",
    "    f1 = f1_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    precision = precision_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    recall = recall_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    accuracy = accuracy_score(y_resampled_test, y_val_pred)\n",
    "    \n",
    "    print(f\"{model_name} Performance on Test Set:\")\n",
    "    print(f\"Macro-F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_resampled_test, y_val_pred))\n",
    "    \n",
    "    # Confusion matrix and analysis on cross-validation predictions\n",
    "    cm = confusion_matrix(y_resampled, y_pred_cv)\n",
    "    print(\"Confusion Matrix (Cross-Validation):\")\n",
    "    print(cm)\n",
    "    \n",
    "    for i, class_label in enumerate(np.unique(y_resampled)):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        print(f\"Class {class_label}: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost\n",
    "model = xgb.XGBClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_val_pred = model.predict(X_resampled_test)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)   # perform cross-validation predictions\n",
    "\n",
    "# Confusion matrix and analysis on cross-validation predictions\n",
    "cm = confusion_matrix(y_resampled, y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Performance on Test Set:\n",
      "Macro-F1 Score: 0.9553\n",
      "Precision: 0.9532\n",
      "Recall: 0.9575\n",
      "Accuracy: 0.9621\n",
      "Confusion Matrix (Cross-Validation): [[35590   942   140]\n",
      " [ 1047 18931   143]\n",
      " [  321   408 40509]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     15360\n",
      "           1       0.91      0.94      0.92      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.96     41078\n",
      "   macro avg       0.95      0.96      0.96     41078\n",
      "weighted avg       0.96      0.96      0.96     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(f\"Confusion Matrix (Cross-Validation): {cm}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "model = lgm.LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "y_val_pred = model.predict(X_resampled_test)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X_resampled, y_resampled, cv=kf)   # perform cross-validation predictions\n",
    "\n",
    "# Confusion matrix and analysis on cross-validation predictions\n",
    "cm = confusion_matrix(y_resampled, y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance on Test Set:\n",
      "Macro-F1 Score: 0.9497\n",
      "Precision: 0.9475\n",
      "Recall: 0.9522\n",
      "Accuracy: 0.9573\n",
      "Confusion Matrix (Cross-Validation): \n",
      "[[35325  1212   135]\n",
      " [ 1297 18680   144]\n",
      " [  387   470 40381]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     15360\n",
      "           1       0.90      0.93      0.91      8315\n",
      "           2       0.99      0.98      0.98     17403\n",
      "\n",
      "    accuracy                           0.96     41078\n",
      "   macro avg       0.95      0.95      0.95     41078\n",
      "weighted avg       0.96      0.96      0.96     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(f\"Confusion Matrix (Cross-Validation): \\n{cm}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Reduced iterations\n",
    "    cv=cv,  # Stratified cross-validation\n",
    "    verbose=1, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Hypertuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [30],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"bootstrap\": [False]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Reduced iterations\n",
    "    cv=cv,  # Stratified cross-validation\n",
    "    verbose=1, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random.fit(X_resampled, y_resampled)\n",
    "y_val_pred = rf_random.predict(X_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance on Test Set:\n",
      "Macro-F1 Score: 0.9602\n",
      "Precision: 0.9572\n",
      "Recall: 0.9635\n",
      "Accuracy: 0.9662\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     15360\n",
      "           1       0.91      0.95      0.93      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.97     41078\n",
      "   macro avg       0.96      0.96      0.96     41078\n",
      "weighted avg       0.97      0.97      0.97     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in pickle file\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_random, model_file)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbqYDMC1kOYxiUNm4o/pqD",
   "mount_file_id": "112yOxEmQ4qFzNIFMqrmQpOIuO5pdNxzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
