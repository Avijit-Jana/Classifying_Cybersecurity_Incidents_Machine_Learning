{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset & Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qbE-4ZdtY7N",
    "outputId": "150e2d3c-afbb-46e9-a049-df0f1e14daeb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../NoteBooks/cleaned_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['IncidentGrade'])  # Features\n",
    "y = df['IncidentGrade']                 # Target variable   \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the dataset size\n",
    "sample_fraction = 0.05  # Adjust as needed (e.g., 10% of the original dataset)\n",
    "df_sampled, _ = train_test_split(\n",
    "    df, \n",
    "    stratify=df['IncidentGrade'], \n",
    "    test_size=1-sample_fraction, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['IncidentGrade'])\n",
    "y = df_sampled['IncidentGrade']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.utils import resample\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def undersample_data(X, y, target_majority_fraction=0.7, random_state=42):\n",
    "\n",
    "    # Get class counts and identify majority and minority classes\n",
    "    class_counts = Counter(y)       \n",
    "    majority_class = max(class_counts, key=class_counts.get)    # get the class with most occurrences\n",
    "    # minority_class = min(class_counts, key=class_counts.get)    # get the class with fewest occurrences\n",
    "    \n",
    "    # Separate majority and non-majority classes\n",
    "    X_majority = X[y == majority_class]\n",
    "    y_majority = y[y == majority_class]\n",
    "    X_minority = X[y != majority_class]\n",
    "    y_minority = y[y != majority_class]\n",
    "\n",
    "    # Calculate target size for majority class\n",
    "    target_majority_size = int(len(y_minority) / (1 - target_majority_fraction))\n",
    "    target_majority_size = min(target_majority_size, len(y_majority))\n",
    "\n",
    "    # Undersample the majority class\n",
    "    X_majority_resampled, y_majority_resampled = resample(\n",
    "        X_majority, y_majority, replace=False,\n",
    "        n_samples=target_majority_size,\n",
    "        random_state=random_state)\n",
    "\n",
    "    # Combine resampled majority with minority classes\n",
    "    X_resampled = pd.concat([X_majority_resampled, X_minority])\n",
    "    y_resampled = pd.concat([y_majority_resampled, y_minority])\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply Undersampling and SMOTE-ENN in chunks\n",
    "chunk_size = 50000  \n",
    "balanced_chunks = []\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)               # Initialize the SMOTE object\n",
    "smote_enn = SMOTEENN(random_state=42, n_jobs=-1)        # Initialize the SMOTE-ENN object\n",
    "\n",
    "for i in range(0, len(X_train), chunk_size):            # Process the data in chunks\n",
    "    X_chunk, y_chunk = X_train[i:i+chunk_size], y_train[i:i+chunk_size]\n",
    "\n",
    "    # Undersample the majority class\n",
    "    X_chunk_undersampled, y_chunk_undersampled = undersample_data(X_chunk, y_chunk)\n",
    "    \n",
    "    # Apply SMOTE to oversample minority classes\n",
    "    X_smote, y_smote = smote.fit_resample(X_chunk_undersampled, y_chunk_undersampled)\n",
    "\n",
    "    # Step 3: Apply SMOTEENN to clean the resampled data\n",
    "    X_res, y_res = smote_enn.fit_resample(X_smote, y_smote)\n",
    "    \n",
    "    # Store the balanced chunk\n",
    "    balanced_chunks.append((X_res, y_res))\n",
    "    \n",
    "# Combine all balanced chunks\n",
    "X_resampled = pd.concat([chunk[0] for chunk in balanced_chunks])    # Combine the X chunks\n",
    "y_resampled = pd.concat([chunk[1] for chunk in balanced_chunks])    # Combine the y chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "def undersample_data(X, y, target_majority_fraction=0.7, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform undersampling on the majority class to balance with minority classes.\n",
    "    \"\"\"\n",
    "    class_counts = Counter(y)\n",
    "    majority_class = max(class_counts, key=class_counts.get)\n",
    "    \n",
    "    # Separate majority and non-majority classes\n",
    "    X_majority = X[y == majority_class]\n",
    "    y_majority = y[y == majority_class]\n",
    "    X_minority = X[y != majority_class]\n",
    "    y_minority = y[y != majority_class]\n",
    "\n",
    "    # Calculate target size for majority class\n",
    "    target_majority_size = int(len(y_minority) / (1 - target_majority_fraction))\n",
    "    target_majority_size = min(target_majority_size, len(y_majority))\n",
    "\n",
    "    # Resample majority class\n",
    "    X_majority_resampled, y_majority_resampled = resample(\n",
    "        X_majority,\n",
    "        y_majority,\n",
    "        replace=False,\n",
    "        n_samples=target_majority_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Combine resampled majority with minority classes\n",
    "    X_resampled = pd.concat([X_majority_resampled, X_minority])\n",
    "    y_resampled = pd.concat([y_majority_resampled, y_minority])\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "def balance_data(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply undersampling, SMOTE, and SMOTEENN to balance the dataset.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    smote_enn = SMOTEENN(random_state=random_state)\n",
    "\n",
    "    # Step 1: Undersample the majority class\n",
    "    X_undersampled, y_undersampled = undersample_data(X, y, random_state=random_state)\n",
    "\n",
    "    # Step 2: Oversample with SMOTE\n",
    "    X_smote, y_smote = smote.fit_resample(X_undersampled, y_undersampled)\n",
    "\n",
    "    # Step 3: Clean with SMOTE-ENN\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X_smote, y_smote)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming X_train and y_train are loaded as Pandas DataFrames/Series\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "X_balanced, y_balanced = balance_data(X_train, y_train)\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_balanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaling 4\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Fit the scalers\n",
    "scaler_standard.fit(X_resampled)\n",
    "scaler_minmax.fit(X_resampled)\n",
    "\n",
    "# Transform the data using the fitted scalers\n",
    "X_standard = scaler_standard.transform(X_resampled)\n",
    "X_minmax = scaler_minmax.transform(X_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"=== Baseline Model: Logistic Regression ===\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_baseline = lr.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Decision Tree\n",
    "print(\"\\n=== Advanced Models: Decision Tree ===\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "print(\"\\n=== Advanced Models: Random Forest (Grid Search) ===\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print(f\"Best Params: {grid_search_rf.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n=== Advanced Models: XGBoost ===\")\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"\\n=== Advanced Models: LightGBM ===\")\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
    "\n",
    "# 3. Cross-Validation\n",
    "print(\"\\n=== Cross-Validation: Logistic Regression ===\")\n",
    "cv_scores = cross_val_score(lr, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Perform Cross-Validation for All Models\n",
    "print(\"=== Cross-Validation Results ===\")\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {cv_scores.mean():.4f}\")\n",
    "    print(f\"  Std Deviation: {cv_scores.std():.4f}\\n\")\n",
    "\n",
    "# Train models on the training set and evaluate on the test set\n",
    "print(\"=== Test Set Results ===\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Classification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Model Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbqYDMC1kOYxiUNm4o/pqD",
   "mount_file_id": "112yOxEmQ4qFzNIFMqrmQpOIuO5pdNxzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
