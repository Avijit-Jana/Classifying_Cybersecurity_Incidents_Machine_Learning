{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset & Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qbE-4ZdtY7N",
    "outputId": "150e2d3c-afbb-46e9-a049-df0f1e14daeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OrgId</th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>AlertId</th>\n",
       "      <th>DetectorId</th>\n",
       "      <th>AlertTitle</th>\n",
       "      <th>IncidentGrade</th>\n",
       "      <th>EvidenceRole</th>\n",
       "      <th>DeviceId</th>\n",
       "      <th>Sha256</th>\n",
       "      <th>...</th>\n",
       "      <th>Category_Impact</th>\n",
       "      <th>Category_InitialAccess</th>\n",
       "      <th>Category_Other</th>\n",
       "      <th>Category_SuspiciousActivity</th>\n",
       "      <th>EntityType_CloudLogonRequest</th>\n",
       "      <th>EntityType_Ip</th>\n",
       "      <th>EntityType_MailMessage</th>\n",
       "      <th>EntityType_Mailbox</th>\n",
       "      <th>EntityType_Other</th>\n",
       "      <th>EntityType_User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>455266534868</td>\n",
       "      <td>88</td>\n",
       "      <td>326</td>\n",
       "      <td>210035</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1056561957389</td>\n",
       "      <td>809</td>\n",
       "      <td>58352</td>\n",
       "      <td>712507</td>\n",
       "      <td>423</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214748368522</td>\n",
       "      <td>148</td>\n",
       "      <td>4359</td>\n",
       "      <td>188041</td>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073741827836</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>831157</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>223338299440</td>\n",
       "      <td>6</td>\n",
       "      <td>2472</td>\n",
       "      <td>1148</td>\n",
       "      <td>17</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98799</td>\n",
       "      <td>138268</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id  OrgId  IncidentId  AlertId  DetectorId  AlertTitle  \\\n",
       "0   455266534868     88         326   210035          58          43   \n",
       "1  1056561957389    809       58352   712507         423         298   \n",
       "2   214748368522    148        4359   188041           9          74   \n",
       "3  1073741827836     72          70   831157           4           3   \n",
       "4   223338299440      6        2472     1148          17         284   \n",
       "\n",
       "   IncidentGrade  EvidenceRole  DeviceId  Sha256  ...  Category_Impact  \\\n",
       "0              0             0     98799  138268  ...            False   \n",
       "1              0             1     98799  138268  ...            False   \n",
       "2              2             0     98799  138268  ...            False   \n",
       "3              2             0     98799  138268  ...            False   \n",
       "4              0             1     98799  138268  ...             True   \n",
       "\n",
       "   Category_InitialAccess  Category_Other  Category_SuspiciousActivity  \\\n",
       "0                   False           False                        False   \n",
       "1                    True           False                        False   \n",
       "2                   False            True                        False   \n",
       "3                    True           False                        False   \n",
       "4                   False           False                        False   \n",
       "\n",
       "   EntityType_CloudLogonRequest  EntityType_Ip  EntityType_MailMessage  \\\n",
       "0                         False          False                   False   \n",
       "1                         False          False                   False   \n",
       "2                         False          False                   False   \n",
       "3                         False          False                   False   \n",
       "4                         False           True                   False   \n",
       "\n",
       "   EntityType_Mailbox  EntityType_Other  EntityType_User  \n",
       "0               False             False             True  \n",
       "1               False              True            False  \n",
       "2               False             False             True  \n",
       "3               False             False             True  \n",
       "4               False             False            False  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../NoteBooks/cleaned_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3437166\n",
      "Validation set size: 1473072\n",
      "\n",
      "Class distribution in training set:\n",
      "IncidentGrade\n",
      "2    0.403098\n",
      "1    0.388515\n",
      "0    0.208387\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "IncidentGrade\n",
      "2    0.403098\n",
      "1    0.388515\n",
      "0    0.208387\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['IncidentGrade'])  # Features\n",
    "y = df['IncidentGrade']                 # Target variable   \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 171857\n",
      "Validation set size: 73654\n",
      "\n",
      "Class distribution in training set:\n",
      "IncidentGrade\n",
      "2    0.403097\n",
      "1    0.388515\n",
      "0    0.208388\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "IncidentGrade\n",
      "2    0.403101\n",
      "1    0.388519\n",
      "0    0.208380\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Reducing the dataset size\n",
    "sample_fraction = 0.05  # Adjust as needed (e.g., 10% of the original dataset)\n",
    "df_sampled, _ = train_test_split(\n",
    "    df, \n",
    "    stratify=df['IncidentGrade'], \n",
    "    test_size=1-sample_fraction, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_sampled.drop(columns=['IncidentGrade'])\n",
    "y = df_sampled['IncidentGrade']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Function to apply balancing in chunks\n",
    "def balance_data(X, y, chunk_size=50000, random_state=42):\n",
    "    balanced_chunks = []  # To store balanced chunks\n",
    "    smote = SMOTE(random_state=random_state, n_jobs=-1)  # Initialize SMOTE\n",
    "    smote_enn = SMOTEENN(random_state=random_state, n_jobs=-1)  # Initialize SMOTE-ENN\n",
    "\n",
    "    for i in range(0, len(X), chunk_size):  # Process data in chunks\n",
    "        X_chunk, y_chunk = X[i:i + chunk_size], y[i:i + chunk_size]\n",
    "\n",
    "        # Step 1: Apply SMOTE to oversample minority classes\n",
    "        X_smote, y_smote = smote.fit_resample(X_chunk, y_chunk)\n",
    "\n",
    "        # Step 2: Apply SMOTEENN to clean noisy samples and further balance\n",
    "        X_res, y_res = smote_enn.fit_resample(X_smote, y_smote)\n",
    "\n",
    "        # Store balanced chunk\n",
    "        balanced_chunks.append((pd.DataFrame(X_res), pd.Series(y_res)))\n",
    "\n",
    "    # Combine all balanced chunks into a single dataset\n",
    "    X_resampled = pd.concat([chunk[0] for chunk in balanced_chunks], ignore_index=True)\n",
    "    y_resampled = pd.concat([chunk[1] for chunk in balanced_chunks], ignore_index=True)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Applying the function to the dataset\n",
    "X_resampled, y_resampled = balance_data(X_train, y_train, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 69275, 1: 66769, 0: 35813})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 41238, 0: 36672, 1: 20121})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_train))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_test, y_resampled_test = balance_data(X_test, y_test, chunk_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 29690, 1: 28616, 0: 15348})\n",
      "Class distribution after undersampling and SMOTEENN: Counter({2: 17403, 0: 15360, 1: 8315})\n"
     ]
    }
   ],
   "source": [
    "# Print class distributions before and after\n",
    "print(\"Original class distribution:\", Counter(y_test))\n",
    "print(\"Class distribution after undersampling and SMOTEENN:\", Counter(y_resampled_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaling 4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = ['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'AlertTitle', 'DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn', 'AccountObjectId', 'AccountName', 'DeviceName', 'NetworkMessageId', 'ApplicationId', 'ApplicationName', 'FileName', 'FolderPath', 'ResourceIdName', 'CountryCode', 'State', 'City', 'Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']\n",
    "\n",
    "# creating a copy of the dataset\n",
    "X_resampled_scaled = X_resampled.copy()     \n",
    "X_resampled_test_scaled = X_resampled_test.copy()\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test datasets\n",
    "X_resampled_scaled[numerical_features] = scaler.fit_transform(X_resampled[numerical_features])  \n",
    "X_resampled_test_scaled[numerical_features] = scaler.transform(X_resampled_test[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Traning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Model with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression Performance on Test Set:\n",
      "Macro-F1 Score: 0.7399\n",
      "Precision: 0.7484\n",
      "Recall: 0.7356\n",
      "Accuracy: 0.7791\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76     15360\n",
      "           1       0.62      0.53      0.57      8315\n",
      "           2       0.90      0.89      0.89     17403\n",
      "\n",
      "    accuracy                           0.78     41078\n",
      "   macro avg       0.75      0.74      0.74     41078\n",
      "weighted avg       0.78      0.78      0.78     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[28528  5511  2633]\n",
      " [ 7624 10647  1850]\n",
      " [ 3561  1065 36612]]\n",
      "Class 0: TP=28528, FP=11185, FN=8144, TN=50174\n",
      "Class 1: TP=10647, FP=6576, FN=9474, TN=71334\n",
      "Class 2: TP=36612, FP=4483, FN=4626, TN=52310\n",
      "--------------------------------------------------\n",
      "Training Decision Tree...\n",
      "Decision Tree Performance on Test Set:\n",
      "Macro-F1 Score: 0.9439\n",
      "Precision: 0.9430\n",
      "Recall: 0.9448\n",
      "Accuracy: 0.9524\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95     15360\n",
      "           1       0.90      0.91      0.90      8315\n",
      "           2       0.98      0.98      0.98     17403\n",
      "\n",
      "    accuracy                           0.95     41078\n",
      "   macro avg       0.94      0.94      0.94     41078\n",
      "weighted avg       0.95      0.95      0.95     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[35101  1275   296]\n",
      " [ 1209 18474   438]\n",
      " [  329   397 40512]]\n",
      "Class 0: TP=35101, FP=1538, FN=1571, TN=59821\n",
      "Class 1: TP=18474, FP=1672, FN=1647, TN=76238\n",
      "Class 2: TP=40512, FP=734, FN=726, TN=56059\n",
      "--------------------------------------------------\n",
      "Training Random Forest...\n",
      "Random Forest Performance on Test Set:\n",
      "Macro-F1 Score: 0.9624\n",
      "Precision: 0.9597\n",
      "Recall: 0.9653\n",
      "Accuracy: 0.9681\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97     15360\n",
      "           1       0.92      0.95      0.93      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.97     41078\n",
      "   macro avg       0.96      0.97      0.96     41078\n",
      "weighted avg       0.97      0.97      0.97     41078\n",
      "\n",
      "Confusion Matrix (Cross-Validation):\n",
      "[[35862   727    83]\n",
      " [  804 19178   139]\n",
      " [  222   529 40487]]\n",
      "Class 0: TP=35862, FP=1026, FN=810, TN=60333\n",
      "Class 1: TP=19178, FP=1256, FN=943, TN=76654\n",
      "Class 2: TP=40487, FP=222, FN=751, TN=56571\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, f1_score, \n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    }\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Perform cross-validation predictions\n",
    "    y_pred_cv = cross_val_predict(model, X_resampled_scaled, y_resampled, cv=kf)\n",
    "    \n",
    "    model.fit(X_resampled_scaled, y_resampled)             # Train on full data\n",
    "    y_val_pred = model.predict(X_resampled_test_scaled)    # Predictions on test data\n",
    "    \n",
    "    # Evaluation metrics on test data\n",
    "    f1 = f1_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    precision = precision_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    recall = recall_score(y_resampled_test, y_val_pred, average=\"macro\")\n",
    "    accuracy = accuracy_score(y_resampled_test, y_val_pred)\n",
    "    \n",
    "    print(f\"{model_name} Performance on Test Set:\")\n",
    "    print(f\"Macro-F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_resampled_test, y_val_pred))\n",
    "    \n",
    "    # Confusion matrix and analysis on cross-validation predictions\n",
    "    cm = confusion_matrix(y_resampled, y_pred_cv)\n",
    "    print(\"Confusion Matrix (Cross-Validation):\")\n",
    "    print(cm)\n",
    "    \n",
    "    for i, class_label in enumerate(np.unique(y_resampled)):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        print(f\"Class {class_label}: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost\n",
    "model = xgb.XGBClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(X_resampled_scaled, y_resampled)\n",
    "y_val_pred = model.predict(X_resampled_test_scaled)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X_resampled_scaled, y_resampled, cv=kf)   # perform cross-validation predictions\n",
    "\n",
    "# Confusion matrix and analysis on cross-validation predictions\n",
    "cm = confusion_matrix(y_resampled, y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Performance on Test Set:\n",
      "Macro-F1 Score: 0.9640\n",
      "Precision: 0.9619\n",
      "Recall: 0.9663\n",
      "Accuracy: 0.9697\n",
      "Confusion Matrix (Cross-Validation): [[35750   797   125]\n",
      " [  776 19201   144]\n",
      " [  202   472 40564]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     15360\n",
      "           1       0.92      0.95      0.94      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.97     41078\n",
      "   macro avg       0.96      0.97      0.96     41078\n",
      "weighted avg       0.97      0.97      0.97     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(f\"Confusion Matrix (Cross-Validation): {cm}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "model = lgm.LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "model.fit(X_resampled_scaled, y_resampled)\n",
    "y_val_pred = model.predict(X_resampled_test_scaled)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = 5\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "y_pred_cv = cross_val_predict(model, X_resampled_scaled, y_resampled, cv=kf)   # perform cross-validation predictions\n",
    "\n",
    "# Confusion matrix and analysis on cross-validation predictions\n",
    "cm = confusion_matrix(y_resampled, y_pred_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance on Test Set:\n",
      "Macro-F1 Score: 0.9575\n",
      "Precision: 0.9547\n",
      "Recall: 0.9606\n",
      "Accuracy: 0.9642\n",
      "Confusion Matrix (Cross-Validation): \n",
      "[[35522  1022   128]\n",
      " [  888 19067   166]\n",
      " [  272   536 40430]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     15360\n",
      "           1       0.91      0.94      0.92      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.96     41078\n",
      "   macro avg       0.95      0.96      0.96     41078\n",
      "weighted avg       0.96      0.96      0.96     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(f\"Confusion Matrix (Cross-Validation): \\n{cm}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter Tuning with RandomizedSearchCV (for Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Reduced iterations\n",
    "    cv=cv,  # Stratified cross-validation\n",
    "    verbose=1, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random.fit(X_resampled_scaled, y_resampled)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Hypertuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the model and parameters for tuning\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [30],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"bootstrap\": [False]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,  # Reduced iterations\n",
    "    cv=cv,  # Stratified cross-validation\n",
    "    verbose=1, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_random.fit(X_resampled_scaled, y_resampled)\n",
    "y_val_pred = rf_random.predict(X_resampled_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance on Test Set:\n",
      "Macro-F1 Score: 0.9643\n",
      "Precision: 0.9616\n",
      "Recall: 0.9674\n",
      "Accuracy: 0.9698\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     15360\n",
      "           1       0.92      0.96      0.94      8315\n",
      "           2       0.99      0.98      0.99     17403\n",
      "\n",
      "    accuracy                           0.97     41078\n",
      "   macro avg       0.96      0.97      0.96     41078\n",
      "weighted avg       0.97      0.97      0.97     41078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_resampled_test, y_val_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_resampled_test, y_val_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_resampled_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in pickle file\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(rf_random, model_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbqYDMC1kOYxiUNm4o/pqD",
   "mount_file_id": "112yOxEmQ4qFzNIFMqrmQpOIuO5pdNxzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
