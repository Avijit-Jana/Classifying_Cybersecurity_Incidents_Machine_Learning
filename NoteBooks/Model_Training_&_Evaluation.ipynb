{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loding Dataset & spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qbE-4ZdtY7N",
    "outputId": "150e2d3c-afbb-46e9-a049-df0f1e14daeb"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../NoteBooks/cleaned_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['IncidentGrade'])  # Features\n",
    "y = df['IncidentGrade']                 # Target variable   \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Check the shape of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Optional: Verify class distribution (use only if stratify=y is set)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Pre-sampling with reduced ratio for majority class\n",
    "sampling_strategy = {class_label: int(0.2 * count) if count > min(Counter(y_train).values()) else count for class_label, count in Counter(y_train).items()}  # Adjust ratio dynamically\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "print(f\"After pre-undersampling: {Counter(y_train_under)}\")\n",
    "\n",
    "# Reduce to 20 principal components (adjust n_components based on variance retained)\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "X_train_reduced = pca.fit_transform(X_train_under)\n",
    "print(f\"Data shape after PCA: {X_train_reduced.shape}\")\n",
    "\n",
    "# smote = SMOTE(sampling_strategy=\"auto\", k_neighbors=5, random_state=42, n_jobs=-1)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train_under, y_train_under)\n",
    "\n",
    "kmeans_smote = KMeansSMOTE(sampling_strategy=\"auto\", random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = kmeans_smote.fit_resample(X_train_under, y_train_under)\n",
    "print(f\"After SMOTE : {Counter(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(f\"Before SMOTE + RandomUnderSampler: {Counter(y_train)}\")\n",
    "\n",
    "# Pre-sampling to reduce dataset size\n",
    "undersampler = RandomUnderSampler(sampling_strategy={class_label: int(0.20*count) for class_label,count in Counter(y_train).items()}, random_state=42)  # Balance classes to 1:0.5 ratio\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"After pre-sampling: {Counter(y_train_under)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(f\"Before SMOTE + RandomUnderSampler: {Counter(y_train)}\")\n",
    "\n",
    "# Step 1: Pre-sampling with reduced ratio for majority class\n",
    "sampling_strategy = {class_label: int(0.3 * count) if count > min(Counter(y_train).values()) else count for class_label, count in Counter(y_train).items()}  # Adjust ratio dynamically\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "print(f\"After pre-sampling: {Counter(y_train_under)}\")\n",
    "\n",
    "# Step 2: SMOTE on reduced dataset\n",
    "smote = SMOTE(sampling_strategy=\"minority\", k_neighbors=2, random_state=42, n_jobs=-1)  # Use fewer neighbors and parallelize\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_under, y_train_under)\n",
    "print(f\"After SMOTE: {Counter(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load dataset (replace with your dataset)\n",
    "# Example: Using sklearn's breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"=== Baseline Model: Logistic Regression ===\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_baseline = lr.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_baseline):.4f}\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Decision Tree\n",
    "print(\"\\n=== Advanced Models: Decision Tree ===\")\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "print(\"\\n=== Advanced Models: Random Forest (Grid Search) ===\")\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "print(f\"Best Params: {grid_search_rf.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n=== Advanced Models: XGBoost ===\")\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "\n",
    "# LightGBM\n",
    "print(\"\\n=== Advanced Models: LightGBM ===\")\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
    "\n",
    "# 3. Cross-Validation\n",
    "print(\"\\n=== Cross-Validation: Logistic Regression ===\")\n",
    "cv_scores = cross_val_score(lr, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Perform Cross-Validation for All Models\n",
    "print(\"=== Cross-Validation Results ===\")\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {cv_scores.mean():.4f}\")\n",
    "    print(f\"  Std Deviation: {cv_scores.std():.4f}\\n\")\n",
    "\n",
    "# Train models on the training set and evaluate on the test set\n",
    "print(\"=== Test Set Results ===\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Test Set Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Classification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbqYDMC1kOYxiUNm4o/pqD",
   "mount_file_id": "112yOxEmQ4qFzNIFMqrmQpOIuO5pdNxzf",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
