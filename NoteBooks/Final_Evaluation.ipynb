{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Dataset/GUIDE_Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.5  # set the threshold for missing values\n",
    "\n",
    "# 1. Remove columns with more than 50% of missing values\n",
    "missing_percentage = df.isnull().mean()\n",
    "columns_to_drop = missing_percentage[missing_percentage > missing_threshold].index\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                    0\n",
       "OrgId                 0\n",
       "IncidentId            0\n",
       "AlertId               0\n",
       "Timestamp             0\n",
       "DetectorId            0\n",
       "AlertTitle            0\n",
       "Category              0\n",
       "IncidentGrade         0\n",
       "EntityType            0\n",
       "EvidenceRole          0\n",
       "DeviceId              0\n",
       "Sha256                0\n",
       "IpAddress             0\n",
       "Url                   0\n",
       "AccountSid            0\n",
       "AccountUpn            0\n",
       "AccountObjectId       0\n",
       "AccountName           0\n",
       "DeviceName            0\n",
       "NetworkMessageId      0\n",
       "RegistryKey           0\n",
       "RegistryValueName     0\n",
       "RegistryValueData     0\n",
       "ApplicationId         0\n",
       "ApplicationName       0\n",
       "OAuthApplicationId    0\n",
       "FileName              0\n",
       "FolderPath            0\n",
       "ResourceIdName        0\n",
       "OSFamily              0\n",
       "OSVersion             0\n",
       "CountryCode           0\n",
       "State                 0\n",
       "City                  0\n",
       "Usage                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df.drop_duplicates(inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing outliers: (3922695, 36)\n",
      "After removing outliers: (2096390, 36)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore    # importing the zscore function\n",
    "import numpy as np                # importing the numpy library \n",
    "\n",
    "# function to remove outliers\n",
    "def remove_outliers(df, threshold=3):\n",
    "    numerical_df = df.select_dtypes(include=[np.number])    # Select only numerical columns\n",
    "    z_scores = np.abs((numerical_df - numerical_df.mean()) / numerical_df.std())    # calculating the zscore\n",
    "\n",
    "    # Filter out rows with Z-scores above the threshold in any column\n",
    "    df_clean = df[(z_scores < threshold).all(axis=1)].copy()\n",
    "    return df_clean     # returning the dataframe\n",
    "\n",
    "print(\"Before removing outliers:\", df.shape)    # printing the shape of the dataframe\n",
    "\n",
    "# Remove outliers using Z-score\n",
    "df = remove_outliers(df)  \n",
    "\n",
    "print(\"After removing outliers:\", df.shape)             # printing the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant Features: ['RegistryValueName', 'RegistryValueData', 'OSFamily', 'OSVersion']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "constant_features = [col for col in numerical_columns if df[col].nunique() == 1]    # finding the constant features\n",
    "print(\"Constant Features:\", constant_features)\n",
    "df.drop(columns=constant_features, inplace=True)    # dropping the constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['RegistryKey','OAuthApplicationId','Usage'], inplace=True, axis=1)    # dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])   # converting the timestamp column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['Timestamp'].dt.year\n",
    "df['Month'] = df['Timestamp'].dt.month\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Minute'] = df['Timestamp'].dt.minute\n",
    "df['Second'] = df['Timestamp'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Timestamp'], inplace=True)    # dropping the timestamp column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Catagorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category         18\n",
       "IncidentGrade     3\n",
       "EntityType       25\n",
       "EvidenceRole      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include=['object']).nunique()    # checking the unique values in the object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df['EvidenceRole'] = le.fit_transform(df['EvidenceRole'])\n",
    "\n",
    "# Ordinal Encoding\n",
    "custom_mapping = {\n",
    "    'FalsePositive': 0,\n",
    "    'BenignPositive': 1,\n",
    "    'TruePositive': 2\n",
    "}\n",
    "df['IncidentGrade'] = df['IncidentGrade'].map(custom_mapping)\n",
    "\n",
    "# One-Hot Encoding\n",
    "# Step 1: Identify the top 4 most frequent categories\n",
    "top_5_categories = df['Category'].value_counts().nlargest(5).index\n",
    "top_5_entities = df['EntityType'].value_counts().nlargest(5).index\n",
    "\n",
    "# Step 2: Create a new column grouping all other categories as 'Other'\n",
    "df['Category_Top'] = df['Category'].apply(lambda x: x if x in top_5_categories else 'Other')\n",
    "df['EntityType_Top'] = df['EntityType'].apply(lambda x: x if x in top_5_entities else 'Other')\n",
    "\n",
    "# Step 3: Perform one-hot encoding on the modified column\n",
    "category_encoded = pd.get_dummies(df['Category_Top'], prefix='Category')\n",
    "df = pd.concat([df, category_encoded], axis=1)\n",
    "entity_encoded = pd.get_dummies(df['EntityType_Top'], prefix='EntityType')\n",
    "df = pd.concat([df, entity_encoded], axis=1)\n",
    "\n",
    "# Step 4: Drop the original category columns if no longer needed\n",
    "df.drop(['Category', 'Category_Top'], axis=1, inplace=True)\n",
    "df.drop(['EntityType', 'EntityType_Top'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scaling 4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the StandardScaler object\n",
    "scaler = pickle.load(open('../NoteBooks/scaler.pkl', 'rb'))\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = ['Id', 'OrgId', 'IncidentId', 'AlertId', 'DetectorId', 'AlertTitle', 'DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn', 'AccountObjectId', 'AccountName', 'DeviceName', 'NetworkMessageId', 'ApplicationId', 'ApplicationName', 'FileName', 'FolderPath', 'ResourceIdName', 'CountryCode', 'State', 'City', 'Year', 'Month', 'Day', 'Hour', 'Minute', 'Second']\n",
    "\n",
    "df1 = df.copy()    # creating a copy of the dataframe\n",
    "\n",
    "df1[numerical_features] = scaler.transform(df1[numerical_features])    # scaling the numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and test on the GUIDE_TEST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance on Test Set:\n",
      "Macro-F1 Score: 0.8293\n",
      "Precision: 0.8319\n",
      "Recall: 0.8335\n",
      "Accuracy: 0.8368\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.79    437337\n",
      "           1       0.77      0.89      0.82    770348\n",
      "           2       0.95      0.81      0.88    888705\n",
      "\n",
      "    accuracy                           0.84   2096390\n",
      "   macro avg       0.83      0.83      0.83   2096390\n",
      "weighted avg       0.85      0.84      0.84   2096390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "# Load the model from the file\n",
    "model = pickle.load(open('../NoteBooks/model.pkl', 'rb'))\n",
    "\n",
    "x = df1.drop(columns=['IncidentGrade'])    # defining the features\n",
    "y_true = df1['IncidentGrade']    # defining the target variable\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Model Performance on Test Set:\")\n",
    "print(f\"Macro-F1 Score: {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
